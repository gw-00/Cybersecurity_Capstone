{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d87a623",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de37fc",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b550742",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libraries here\n",
    "# Data handling\n",
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc98310",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "First, we will use the function 'glob' because it will return all the files starting with \"merged\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e44ac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:  63\n"
     ]
    }
   ],
   "source": [
    "# Merged Files Path\n",
    "path = \"../Data/Raw\"\n",
    "\n",
    "# Grabbing the correct path\n",
    "raw_data_path = os.path.join(path, \"Merged*.csv\")\n",
    "\n",
    "# Putting all the chunks in an array\n",
    "merged_files = sorted(glob.glob(raw_data_path))\n",
    "\n",
    "print(\"Found Files: \", len(merged_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f7697",
   "metadata": {},
   "source": [
    "Using Polars to handle big datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b3580db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 40)\n",
      "┌────────────┬────────────┬────────────┬────────────┬───┬──────────┬────────┬──────────┬───────────┐\n",
      "│ Header_Len ┆ Protocol   ┆ Time_To_Li ┆ Rate       ┆ … ┆ IAT      ┆ Number ┆ Variance ┆ Label     │\n",
      "│ gth        ┆ Type       ┆ ve         ┆ ---        ┆   ┆ ---      ┆ ---    ┆ ---      ┆ ---       │\n",
      "│ ---        ┆ ---        ┆ ---        ┆ f64        ┆   ┆ f64      ┆ i64    ┆ f64      ┆ str       │\n",
      "│ f64        ┆ i64        ┆ f64        ┆            ┆   ┆          ┆        ┆          ┆           │\n",
      "╞════════════╪════════════╪════════════╪════════════╪═══╪══════════╪════════╪══════════╪═══════════╡\n",
      "│ 19.92      ┆ 6          ┆ 63.36      ┆ 25893.9622 ┆ … ┆ 0.000039 ┆ 100    ┆ 1772.41  ┆ DDOS-PSHA │\n",
      "│            ┆            ┆            ┆ 18         ┆   ┆          ┆        ┆          ┆ CK_FLOOD  │\n",
      "│ 0.0        ┆ 47         ┆ 64.0       ┆ 3703.84133 ┆ … ┆ 0.000271 ┆ 100    ┆ 2304.0   ┆ MIRAI-GRE │\n",
      "│            ┆            ┆            ┆ 1          ┆   ┆          ┆        ┆          ┆ IP_FLOOD  │\n",
      "└────────────┴────────────┴────────────┴────────────┴───┴──────────┴────────┴──────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Build a single lazy frame from all CSVs\n",
    "lazy_frames = [pl.scan_csv(f) for f in merged_files]\n",
    "\n",
    "# Concatenate lazily\n",
    "raw_lf = pl.concat(lazy_frames)\n",
    "\n",
    "# Quick sanity check on just a couple of rows\n",
    "print(raw_lf.limit(2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e04ea",
   "metadata": {},
   "source": [
    "### 1.1 Saving Raw dataset\n",
    "The format will be Parquet because it is faster than working with CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaa05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset saved to Parquet format.\n"
     ]
    }
   ],
   "source": [
    "# Stream the full lazy dataset directly to Parquet without loading everything into RAM\n",
    "raw_lf.sink_parquet(\"../Data/Raw/Raw_Dataset\")\n",
    "print(\"Raw dataset saved to Parquet format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86819f",
   "metadata": {},
   "source": [
    "### 1.2 Loading Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98dcf109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scan_parquet to keep things lazy\n",
    "raw_lf = pl.scan_parquet(\"../Data/Raw/Raw_Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf42a1",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8085010",
   "metadata": {},
   "source": [
    "Dropping null values in 'Label' and 'Std' columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e66471",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lf = raw_lf.drop_nulls([\"Label\", \"Std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b30197",
   "metadata": {},
   "source": [
    "Adding in the 'Attack_Family' and 'Binary_Label' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d658b002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COMMANDINJECTION',\n",
       " 'XSS',\n",
       " 'DDOS-ICMP_FLOOD',\n",
       " 'DDOS-ACK_FRAGMENTATION',\n",
       " 'DDOS-SYN_FLOOD',\n",
       " 'DOS-UDP_FLOOD',\n",
       " 'VULNERABILITYSCAN',\n",
       " 'DNS_SPOOFING',\n",
       " 'DDOS-SLOWLORIS',\n",
       " 'BENIGN',\n",
       " 'DOS-HTTP_FLOOD',\n",
       " 'MITM-ARPSPOOFING',\n",
       " 'DDOS-PSHACK_FLOOD',\n",
       " 'RECON-OSSCAN',\n",
       " 'MIRAI-UDPPLAIN',\n",
       " 'BACKDOOR_MALWARE',\n",
       " 'RECON-PINGSWEEP',\n",
       " 'DDOS-UDP_FRAGMENTATION',\n",
       " 'RECON-HOSTDISCOVERY',\n",
       " 'DDOS-UDP_FLOOD',\n",
       " 'DDOS-RSTFINFLOOD',\n",
       " 'DDOS-HTTP_FLOOD',\n",
       " 'BROWSERHIJACKING',\n",
       " 'DICTIONARYBRUTEFORCE',\n",
       " 'DDOS-ICMP_FRAGMENTATION',\n",
       " 'DDOS-SYNONYMOUSIP_FLOOD',\n",
       " 'MIRAI-GREETH_FLOOD',\n",
       " 'UPLOADING_ATTACK',\n",
       " 'DOS-TCP_FLOOD',\n",
       " 'DDOS-TCP_FLOOD',\n",
       " 'DOS-SYN_FLOOD',\n",
       " 'SQLINJECTION',\n",
       " 'RECON-PORTSCAN',\n",
       " 'MIRAI-GREIP_FLOOD']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect unique attack names\n",
    "unique_labels = (\n",
    "    clean_lf\n",
    "    .select(pl.col(\"Label\").unique())\n",
    "    .collect()\n",
    "    .to_pandas()[\"Label\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3560764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique attack families are: shape: (8, 1)\n",
      "┌──────────────┐\n",
      "│ Label_Family │\n",
      "│ ---          │\n",
      "│ str          │\n",
      "╞══════════════╡\n",
      "│ BENIGN       │\n",
      "│ DOS          │\n",
      "│ RECON        │\n",
      "│ DDOS         │\n",
      "│ BRUTE_FORCE  │\n",
      "│ WEB          │\n",
      "│ SPOOFING     │\n",
      "│ MIRAI        │\n",
      "└──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# mapping attack labels to their attack family\n",
    "label_to_family = {\n",
    "    # BENIGN\n",
    "    \"BENIGN\": \"BENIGN\",\n",
    "\n",
    "    # DDoS Family\n",
    "    \"DDOS-ACK_FRAGMENTATION\": \"DDOS\",\n",
    "    \"DDOS-UDP_FLOOD\": \"DDOS\",\n",
    "    \"DDOS-SLOWLORIS\": \"DDOS\",\n",
    "    \"DDOS-ICMP_FLOOD\": \"DDOS\",\n",
    "    \"DDOS-RSTFINFLOOD\": \"DDOS\",\n",
    "    \"DDOS-PSHACK_FLOOD\": \"DDOS\",\n",
    "    \"DDOS-HTTP_FLOOD\": \"DDOS\",\n",
    "    \"DDOS-UDP_FRAGMENTATION\": \"DDOS\",\n",
    "    \"DDOS-TCP_FLOOD\": \"DDOS\",\n",
    "    \"DDOS-SYN_FLOOD\": \"DDOS\",\n",
    "    \"DDOS-SYNONYMOUSIP_FLOOD\": \"DDOS\",\n",
    "    \"DDOS-ICMP_FRAGMENTATION\": \"DDOS\",\n",
    "\n",
    "    # DoS Family\n",
    "    \"DOS-TCP_FLOOD\": \"DOS\",\n",
    "    \"DOS-HTTP_FLOOD\": \"DOS\",\n",
    "    \"DOS-SYN_FLOOD\": \"DOS\",\n",
    "    \"DOS-UDP_FLOOD\": \"DOS\",\n",
    "\n",
    "    # Recon Family\n",
    "    \"RECON-PINGSWEEP\": \"RECON\",\n",
    "    \"RECON-OSSCAN\": \"RECON\",\n",
    "    \"RECON-PORTSCAN\": \"RECON\",\n",
    "    \"RECON-HOSTDISCOVERY\": \"RECON\",\n",
    "    \"VULNERABILITYSCAN\": \"RECON\",\n",
    "\n",
    "    # Brute Force\n",
    "    \"DICTIONARYBRUTEFORCE\": \"BRUTE_FORCE\",\n",
    "\n",
    "    # Spoofing\n",
    "    \"MITM-ARPSPOOFING\": \"SPOOFING\",\n",
    "    \"DNS_SPOOFING\": \"SPOOFING\",\n",
    "\n",
    "    # Web-based\n",
    "    \"SQLINJECTION\": \"WEB\",\n",
    "    \"COMMANDINJECTION\": \"WEB\",\n",
    "    \"BACKDOOR_MALWARE\": \"WEB\",\n",
    "    \"UPLOADING_ATTACK\": \"WEB\",\n",
    "    \"XSS\": \"WEB\",\n",
    "    \"BROWSERHIJACKING\": \"WEB\",\n",
    "\n",
    "    # Mirai\n",
    "    \"MIRAI-GREIP_FLOOD\": \"MIRAI\",\n",
    "    \"MIRAI-GREETH_FLOOD\": \"MIRAI\",\n",
    "    \"MIRAI-UDPPLAIN\": \"MIRAI\",\n",
    "}\n",
    "\n",
    "# adding the attack family column\n",
    "clean_lf = clean_lf.with_columns(\n",
    "    pl.col(\"Label\").replace(label_to_family).alias(\"Label_Family\")\n",
    ")\n",
    "# inspect unique attack families\n",
    "print(\"The unique attack families are:\", clean_lf.select(pl.col(\"Label_Family\").unique()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e4ac39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique binary labels are: shape: (2, 1)\n",
      "┌──────────────┐\n",
      "│ Label_Binary │\n",
      "│ ---          │\n",
      "│ i32          │\n",
      "╞══════════════╡\n",
      "│ 0            │\n",
      "│ 1            │\n",
      "└──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# creation of the binary attack label column from the attack family\n",
    "clean_lf = clean_lf.with_columns(\n",
    "    pl.when(pl.col(\"Label_Family\") == \"BENIGN\")\n",
    "      .then(0)\n",
    "      .otherwise(1)\n",
    "      .alias(\"Label_Binary\")\n",
    ")\n",
    "\n",
    "# inspect unique binary labels\n",
    "print(\"The unique binary labels are:\", clean_lf.select(pl.col(\"Label_Binary\").unique()).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2450c",
   "metadata": {},
   "source": [
    "Removing Duplicates\n",
    "This can take a while due to the dataset size and will eat a lot of RAM. Please make sure to have 16GB of RAM available for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f254d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lf = clean_lf.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db551d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dedupe: 21005417\n"
     ]
    }
   ],
   "source": [
    "rows_after_dedupe = clean_lf.select(pl.len()).collect().item()\n",
    "print(\"Rows after dedupe:\", rows_after_dedupe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca23e9",
   "metadata": {},
   "source": [
    "Dropping Infinite Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe5b630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lf = clean_lf.filter(~pl.col(\"Rate\").is_infinite())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79196197",
   "metadata": {},
   "source": [
    "Cleaning up column names with consistent formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23271ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names: replace spaces and capitalization inconsistencies\n",
    "clean_lf = clean_lf.rename({\n",
    "    col: col.strip().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    for col in clean_lf.collect_schema().names()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ca118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header_Length', 'Protocol_Type', 'Time_To_Live', 'Rate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', 'IPv', 'LLC', 'Tot_sum', 'Min', 'Max', 'AVG', 'Std', 'Tot_size', 'IAT', 'Number', 'Variance', 'Label', 'Label_Family', 'Label_Binary']\n"
     ]
    }
   ],
   "source": [
    "# Print all column names without collecting the dataset\n",
    "column_names = clean_lf.collect_schema().names()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d86e5",
   "metadata": {},
   "source": [
    "Inspect the data types of each column in the lazy df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9934de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header_Length: Float64\n",
      "Protocol_Type: Int64\n",
      "Time_To_Live: Float64\n",
      "Rate: Float64\n",
      "fin_flag_number: Float64\n",
      "syn_flag_number: Float64\n",
      "rst_flag_number: Float64\n",
      "psh_flag_number: Float64\n",
      "ack_flag_number: Float64\n",
      "ece_flag_number: Float64\n",
      "cwr_flag_number: Float64\n",
      "ack_count: Int64\n",
      "syn_count: Int64\n",
      "fin_count: Int64\n",
      "rst_count: Int64\n",
      "HTTP: Float64\n",
      "HTTPS: Float64\n",
      "DNS: Float64\n",
      "Telnet: Float64\n",
      "SMTP: Float64\n",
      "SSH: Float64\n",
      "IRC: Float64\n",
      "TCP: Float64\n",
      "UDP: Float64\n",
      "DHCP: Float64\n",
      "ARP: Float64\n",
      "ICMP: Float64\n",
      "IGMP: Float64\n",
      "IPv: Float64\n",
      "LLC: Float64\n",
      "Tot_sum: Int64\n",
      "Min: Int64\n",
      "Max: Int64\n",
      "AVG: Float64\n",
      "Std: Float64\n",
      "Tot_size: Float64\n",
      "IAT: Float64\n",
      "Number: Int64\n",
      "Variance: Float64\n",
      "Label: String\n",
      "Label_Family: String\n",
      "Label_Binary: Int32\n"
     ]
    }
   ],
   "source": [
    "schema = clean_lf.collect_schema()\n",
    "\n",
    "for col, dtype in schema.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b372ca54",
   "metadata": {},
   "source": [
    "Setting the correct column types for each feature and converting to Int32 and Float32.\n",
    "\n",
    "** Please note that converting to categorical causes null values to appear using Polars. To handle this we will keep the categorical columns as strings to ensure downstream compatibility and we will convert to categorical if needed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c17a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = [\"ack_count\", \"syn_count\", \"fin_count\", \"rst_count\", \"Tot_sum\", \"Min\", \"Max\", \"Number\"]\n",
    "\n",
    "float_cols = [\"Header_Length\", \"Time_To_Live\", \"Rate\", \"fin_flag_number\", \"syn_flag_number\", \"rst_flag_number\",\n",
    "              \"psh_flag_number\", \"ack_flag_number\", \"ece_flag_number\", \"cwr_flag_number\", \"HTTP\", \"HTTPS\", \"DNS\",\n",
    "              \"Telnet\", \"SMTP\", \"SSH\", \"IRC\", \"TCP\", \"UDP\", \"DHCP\", \"ARP\", \"ICMP\", \"IGMP\", \"IPv\", \"LLC\", \"AVG\", \n",
    "              \"Std\", \"Tot_size\", \"IAT\", \"Variance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b025c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lf = clean_lf.with_columns(\n",
    "    # cast integers to Int32\n",
    "    [pl.col(c).cast(pl.Int32) for c in int_cols] +\n",
    "    # cast continuous features to Float32\n",
    "    [pl.col(c).cast(pl.Float32) for c in float_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8274d7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header_Length: Float32\n",
      "Protocol_Type: Int64\n",
      "Time_To_Live: Float32\n",
      "Rate: Float32\n",
      "fin_flag_number: Float32\n",
      "syn_flag_number: Float32\n",
      "rst_flag_number: Float32\n",
      "psh_flag_number: Float32\n",
      "ack_flag_number: Float32\n",
      "ece_flag_number: Float32\n",
      "cwr_flag_number: Float32\n",
      "ack_count: Int32\n",
      "syn_count: Int32\n",
      "fin_count: Int32\n",
      "rst_count: Int32\n",
      "HTTP: Float32\n",
      "HTTPS: Float32\n",
      "DNS: Float32\n",
      "Telnet: Float32\n",
      "SMTP: Float32\n",
      "SSH: Float32\n",
      "IRC: Float32\n",
      "TCP: Float32\n",
      "UDP: Float32\n",
      "DHCP: Float32\n",
      "ARP: Float32\n",
      "ICMP: Float32\n",
      "IGMP: Float32\n",
      "IPv: Float32\n",
      "LLC: Float32\n",
      "Tot_sum: Int32\n",
      "Min: Int32\n",
      "Max: Int32\n",
      "AVG: Float32\n",
      "Std: Float32\n",
      "Tot_size: Float32\n",
      "IAT: Float32\n",
      "Number: Int32\n",
      "Variance: Float32\n",
      "Label: String\n",
      "Label_Family: String\n",
      "Label_Binary: Int32\n"
     ]
    }
   ],
   "source": [
    "# confirm data type changes\n",
    "for name, dtype in clean_lf.collect_schema().items():\n",
    "    print(f\"{name}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef0db8",
   "metadata": {},
   "source": [
    "Examine the first few rows of the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94beefd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to: ../Data/Cleaned/Cleaned_Dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "# cleaned dataset to Parquet (streaming, memory-safe)\n",
    "output_path = \"../Data/Cleaned/Cleaned_Dataset.parquet\"\n",
    "\n",
    "clean_lf.sink_parquet(output_path)\n",
    "\n",
    "print(f\"Cleaned dataset written to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
